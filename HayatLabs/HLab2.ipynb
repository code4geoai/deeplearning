{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d6888ef",
   "metadata": {},
   "source": [
    "# Introuduction:\n",
    "This is the lab-2 introducing the basic version of a Deep Learning Model implenentation in pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856aad4e",
   "metadata": {},
   "source": [
    "# Class Inheritence\n",
    "This section introduces the basic strucure of class inheritance which we will follow in pytorch for inheriting our classes from torch.nn.Modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db337913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructor of Parent Class\n",
      "Constructor of Child Class\n"
     ]
    }
   ],
   "source": [
    "class parent ():\n",
    "    def __init__(self):\n",
    "        print(\"Constructor of Parent Class\")\n",
    "class child(parent):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"Constructor of Child Class\")\n",
    "\n",
    "obj = child()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554185a6",
   "metadata": {},
   "source": [
    "Now Lets define a DenseLayer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2f690a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8005edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ourDenseLayer(torch.nn.Module):\n",
    "    def __init__(self,num_input,num_output):\n",
    "        super(ourDenseLayer, self).__init__()\n",
    "        self.w = torch.nn.Parameter(torch.rand(num_input,num_output))\n",
    "        self.bias = torch.nn.Parameter(torch.rand(num_output))\n",
    "    def forward(self,x):\n",
    "        z = torch.matmul(x, self.w) +self.bias\n",
    "        y = torch.sigmoid(z)\n",
    "        return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d4652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.w: Parameter containing:\n",
      "tensor([[0.3270, 0.3158, 0.8558],\n",
      "        [0.3883, 0.5105, 0.5351]], requires_grad=True) having shape: torch.Size([2, 3]) is 2-dimensional\n",
      "layer.bias: Parameter containing:\n",
      "tensor([0.3450, 0.5998, 0.4409], requires_grad=True) having shape torch.Size([3]) is 1-dimensional\n",
      "x_input: tensor([[1., 2.]]) having shape:torch.Size([1, 2]) is 2-dimensional\n",
      "y output: tensor([[0.8098, 0.8740, 0.9143]], grad_fn=<SigmoidBackward0>) having shape:torch.Size([1, 3]) is 2-dimensional\n"
     ]
    }
   ],
   "source": [
    "num_input = 2\n",
    "num_output = 3\n",
    "layer = ourDenseLayer(num_input,num_output)\n",
    "print(f\"layer.w: {layer.w} having shape: {layer.w.shape} is {layer.w.dim()}-dimensional\")\n",
    "print(f\"layer.bias: {layer.bias} having shape {layer.bias.shape} is {layer.bias.dim()}-dimensional\")\n",
    "\n",
    "x_input= torch.tensor([[1,2.]])\n",
    "print(f\"x_input: {x_input} having shape:{x_input.shape} is {x_input.dim()}-dimensional\")\n",
    "\n",
    "y=layer(x_input)\n",
    "print(f\"y output: {y} having shape:{y.shape} is {y.dim()}-dimensional\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e4dda4",
   "metadata": {},
   "source": [
    "# Defining NN using Sequential\n",
    "NOw defining a NN in pytorch is somehow automatic and not manual as shown in ourDenseLayer example. Now instead of using single module we will use sequential which act as container for multiple layers (linear and non-linear). In the example below it takes a linear layer followed by non-linear(sigmoid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e63ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_input:tensor([[1., 2., 3.]]) having shapetorch.Size([1, 3]) is 2-dimensional\n",
      " y_output: tensor([[0.8024, 0.1951, 0.3637, 0.6053]], grad_fn=<SigmoidBackward0>) having shape:torch.Size([1, 4]) is 2-dimensional\n"
     ]
    }
   ],
   "source": [
    "input_nodes = 3 # This is the Weights W which will be multiplied with x_input so their dimensions should match. W*x_input.\n",
    "output_nodes = 4\n",
    "model = nn.Sequential(nn.Linear(input_nodes,output_nodes),nn.Sigmoid())\n",
    "x_input = torch.tensor([[1,2,3.]])\n",
    "y = model(x_input)\n",
    "\n",
    "\n",
    "print(f\"x_input:{x_input} having shape{x_input.shape} is {x_input.dim()}-dimensional\")\n",
    "print(f\" y_output: {y} having shape:{y.shape} is {y.dim()}-dimensional\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedbd779",
   "metadata": {},
   "source": [
    "Although using the `nn.Sequential` is simple but if you want custom architecture, custom layers, custom forward pass  and custom activation function then we can use the `nn.Module` to create a subclass as we have seen in ourDenseLayer example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4adbd8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearWithSigmoid(nn.Module):\n",
    "    def __init__(self,num_inputs,num_outputs):\n",
    "        super().__init__()\n",
    "        #super(lineraWithSigmoid, self).__init__() # This is an older way of calling the parent class constructor\n",
    "        self.linear = nn.Linear(num_inputs,num_outputs)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        z = self.linear(inputs)\n",
    "        y = self.activation(z)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48aa7c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_input(X) shape : torch.Size([1, 3]) is 2-dimensional\n",
      " y_outputshape:torch.Size([1, 4]) is 2-dimensional\n"
     ]
    }
   ],
   "source": [
    "input_nodes = 3 # This is the Weights W which will be multiplied with x_input so their dimensions should match. W*x_input.\n",
    "output_nodes = 4\n",
    "layer = linearWithSigmoid(input_nodes,output_nodes)\n",
    "x_input = torch.tensor([[1,2,3.]])\n",
    "y = layer(x_input)\n",
    "print(f\"x_input(X) shape : {x_input.shape} is {x_input.dim()}-dimensional\")\n",
    "print(f\" y_outputshape:{y.shape} is {y.dim()}-dimensional\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
