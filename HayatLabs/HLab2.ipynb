{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d6888ef",
   "metadata": {},
   "source": [
    "# Introuduction:\n",
    "This is the lab-2 introducing the basic version of a Deep Learning Model implenentation in pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856aad4e",
   "metadata": {},
   "source": [
    "# Class Inheritence\n",
    "This section introduces the basic strucure of class inheritance which we will follow in pytorch for inheriting our classes from torch.nn.Modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db337913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructor of Parent Class\n",
      "Constructor of Child Class\n"
     ]
    }
   ],
   "source": [
    "class parent ():\n",
    "    def __init__(self):\n",
    "        print(\"Constructor of Parent Class\")\n",
    "class child(parent):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"Constructor of Child Class\")\n",
    "\n",
    "obj = child()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554185a6",
   "metadata": {},
   "source": [
    "Now Lets define a DenseLayer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f690a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8005edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ourDenseLayer(torch.nn.Module):\n",
    "    def __init__(self,num_input,num_output):\n",
    "        super(ourDenseLayer, self).__init__()\n",
    "        self.w = torch.nn.Parameter(torch.rand(num_input,num_output))\n",
    "        self.bias = torch.nn.Parameter(torch.rand(num_output))\n",
    "    def forward(self,x):\n",
    "        z = torch.matmul(x, self.w) +self.bias\n",
    "        y = torch.sigmoid(z)\n",
    "        return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d4652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.w: Parameter containing:\n",
      "tensor([[0.3270, 0.3158, 0.8558],\n",
      "        [0.3883, 0.5105, 0.5351]], requires_grad=True) having shape: torch.Size([2, 3]) is 2-dimensional\n",
      "layer.bias: Parameter containing:\n",
      "tensor([0.3450, 0.5998, 0.4409], requires_grad=True) having shape torch.Size([3]) is 1-dimensional\n",
      "x_input: tensor([[1., 2.]]) having shape:torch.Size([1, 2]) is 2-dimensional\n",
      "y output: tensor([[0.8098, 0.8740, 0.9143]], grad_fn=<SigmoidBackward0>) having shape:torch.Size([1, 3]) is 2-dimensional\n"
     ]
    }
   ],
   "source": [
    "num_input = 2\n",
    "num_output = 3\n",
    "layer = ourDenseLayer(num_input,num_output)\n",
    "print(f\"layer.w: {layer.w} having shape: {layer.w.shape} is {layer.w.dim()}-dimensional\")\n",
    "print(f\"layer.bias: {layer.bias} having shape {layer.bias.shape} is {layer.bias.dim()}-dimensional\")\n",
    "\n",
    "x_input= torch.tensor([[1,2.]])\n",
    "print(f\"x_input: {x_input} having shape:{x_input.shape} is {x_input.dim()}-dimensional\")\n",
    "\n",
    "y=layer(x_input)\n",
    "print(f\"y output: {y} having shape:{y.shape} is {y.dim()}-dimensional\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e4dda4",
   "metadata": {},
   "source": [
    "# Defining NN using Sequential\n",
    "NOw defining a NN in pytorch is somehow automatic and not manual as shown in ourDenseLayer example. Now instead of using single module we will use sequential which act as container for multiple layers (linear and non-linear). In the example below it takes a linear layer followed by non-linear(sigmoid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e63ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_input:tensor([[1., 2., 3.]]) having shapetorch.Size([1, 3]) is 2-dimensional\n",
      " y_output: tensor([[0.8024, 0.1951, 0.3637, 0.6053]], grad_fn=<SigmoidBackward0>) having shape:torch.Size([1, 4]) is 2-dimensional\n"
     ]
    }
   ],
   "source": [
    "input_nodes = 3 # This is the Weights W which will be multiplied with x_input so their dimensions should match. W*x_input.\n",
    "output_nodes = 4\n",
    "model = nn.Sequential(nn.Linear(input_nodes,output_nodes),nn.Sigmoid())\n",
    "x_input = torch.tensor([[1,2,3.]])\n",
    "y = model(x_input)\n",
    "\n",
    "\n",
    "print(f\"x_input:{x_input} having shape{x_input.shape} is {x_input.dim()}-dimensional\")\n",
    "print(f\" y_output: {y} having shape:{y.shape} is {y.dim()}-dimensional\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedbd779",
   "metadata": {},
   "source": [
    "Although using the `nn.Sequential` is simple but if you want custom architecture, custom layers, custom forward pass  and custom activation function then we can use the `nn.Module` to create a subclass as we have seen in ourDenseLayer example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4adbd8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearWithSigmoid(nn.Module):\n",
    "    def __init__(self,num_inputs,num_outputs):\n",
    "        super().__init__()\n",
    "        #super(lineraWithSigmoid, self).__init__() # This is an older way of calling the parent class constructor\n",
    "        self.linear = nn.Linear(num_inputs,num_outputs)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        z = self.linear(inputs)\n",
    "        y = self.activation(z)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48aa7c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_input(X) shape : torch.Size([1, 3]) is 2-dimensional\n",
      " y_outputshape:torch.Size([1, 4]) is 2-dimensional\n"
     ]
    }
   ],
   "source": [
    "input_nodes = 3 # This is the Weights W which will be multiplied with x_input so their dimensions should match. W*x_input.\n",
    "output_nodes = 4\n",
    "layer = linearWithSigmoid(input_nodes,output_nodes)\n",
    "x_input = torch.tensor([[1,2,3.]])\n",
    "y = layer(x_input)\n",
    "print(f\"x_input(X) shape : {x_input.shape} is {x_input.dim()}-dimensional\")\n",
    "print(f\" y_outputshape:{y.shape} is {y.dim()}-dimensional\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834cf87",
   "metadata": {},
   "source": [
    "# Model Returing Input as Output\n",
    "Sometime we may want out model to return the input as output without any purterbation or change. We can implement this type of behaviour in forward pass. Suppose in this example we have a boolean variable `isidentity`  to control this behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65e151d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearWithIdentity(nn.Module):\n",
    "    def __init__(self,num_inputs,num_outputs):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_inputs,num_outputs)\n",
    "    def forward(self,num_inputs,isidentity = False):\n",
    "        if isidentity:\n",
    "            return num_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be807020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input :tensor([[[1., 2., 3.]]]) and output: tensor([[[1., 2., 3.]]])\n"
     ]
    }
   ],
   "source": [
    "input_nodes = 3\n",
    "output_nodes = 4\n",
    "\n",
    "model = linearWithIdentity(input_nodes,output_nodes)\n",
    "x_input = torch.tensor([[[1,2,3.]]])\n",
    "\n",
    "y = model(x_input, isidentity = True)\n",
    "\n",
    "print(f\"Input :{x_input} and output: {y}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f831bc",
   "metadata": {},
   "source": [
    "# Backpropatation\n",
    "In PyTorch, when a tensor has requires_grad=True, it means that operations involving this tensor will be tracked for automatic differentiation. During the backward pass (backpropagation), gradients — which are the partial derivatives of a loss (or output) with respect to the input tensor — are computed and stored in the .grad attribute of that tensor.\n",
    "Suppose x is an input tensor and y is some output tensor (e.g., a loss value). If x was defined with requires_grad=True, then calling y.backward() will compute the gradient of y with respect to x, and store it in x.grad.\n",
    "Lets compute the gradient of y i.e.  \n",
    "$y = x^2 $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ce8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy_dx of y=x^2 at x=3.0 is:  6.0\n",
      "The y is 9.0 and dy_dx is 6.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.,requires_grad= True)\n",
    "y = x**2\n",
    "# Now to compute the gradient of y with respect to x, we call the backward method on y.\n",
    "y.backward()\n",
    "# The gradient of y with respect to x is now stored in x.grad\n",
    "dy_dx = x.grad\n",
    "print(\"dy_dx of y=x^2 at x=3.0 is: \", dy_dx.item())\n",
    "print(f'The y is {y} and dy_dx is {dy_dx}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc3849c",
   "metadata": {},
   "source": [
    "# Loss Function:\n",
    "In NN we use differentiation and stogastic gradient descent to optimize the loss function. Let's use an example to find the minimum of loss function  \n",
    "$L = (x-x_f)^2$  \n",
    "Here x_f is the variable for a desire value we are trying to optimize for. L is the loss function we are trying to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c37ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniliazing x with random value: 0.3435054421424866\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1)\n",
    "print(f\"Iniliazing x with random value: {x.item()}\")\n",
    "\n",
    "learning_rate = 1e-2\n",
    "history = []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
